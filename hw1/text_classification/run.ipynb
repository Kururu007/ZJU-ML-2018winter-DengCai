{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. Please check the pdf file for more details.*\n",
    "\n",
    "In this exercise you will:\n",
    "    \n",
    "- implement a of spam classifier with **Naive Bayes method** for real world email messages\n",
    "- learn the **training and testing phase** for Naive Bayes classifier  \n",
    "- get an idea of the **precision-recall** tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msc/anaconda3/envs/kururudev-tf/lib/python3.6/site-packages/scipy/sparse/compressed.py:746: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# ham_train contains the occurrences of each word in ham emails. 1-by-N vector\n",
    "ham_train = np.loadtxt('ham_train.csv', delimiter=',')\n",
    "# print(type(ham_train), len(ham_train), ham_train.shape)\n",
    "# spam_train contains the occurrences of each word in spam emails. 1-by-N vector\n",
    "spam_train = np.loadtxt('spam_train.csv', delimiter=',')\n",
    "# N is the size of vocabulary.\n",
    "N = ham_train.shape[0]\n",
    "# There 9034 ham emails and 3372 spam emails in the training samples\n",
    "num_ham_train = 9034\n",
    "num_spam_train = 3372\n",
    "# Do smoothing\n",
    "x = np.vstack([ham_train, spam_train]) + 1\n",
    "\n",
    "# ham_test contains the occurences of each word in each ham test email. P-by-N vector, with P is number of ham test emails.\n",
    "i,j,ham_test = np.loadtxt('ham_test.txt').T\n",
    "# change interpreter to python3\n",
    "i = i.astype(int)\n",
    "j = j.astype(int)\n",
    "ham_test = ham_test.astype(int)\n",
    "ham_test_tight = scipy.sparse.coo_matrix((ham_test, (i - 1, j - 1)))\n",
    "ham_test = scipy.sparse.csr_matrix((ham_test_tight.shape[0], ham_train.shape[0]))\n",
    "ham_test[:, 0:ham_test_tight.shape[1]] = ham_test_tight\n",
    "# spam_test contains the occurences of each word in each spam test email. Q-by-N vector, with Q is number of spam test emails.\n",
    "i,j,spam_test = np.loadtxt('spam_test.txt').T\n",
    "# change interpreter to python3\n",
    "i = i.astype(int)\n",
    "j = j.astype(int)\n",
    "spam_test = spam_test.astype(int)\n",
    "spam_test_tight = scipy.sparse.csr_matrix((spam_test, (i - 1, j - 1)))\n",
    "spam_test = scipy.sparse.csr_matrix((spam_test_tight.shape[0], spam_train.shape[0]))\n",
    "spam_test[:, 0:spam_test_tight.shape[1]] = spam_test_tight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_words_dict(path):\n",
    "    '''\n",
    "    根据all_word_map.txt文件返回字典\n",
    "    :param path: 文件路径\n",
    "    :return: word_dict\n",
    "    '''\n",
    "    file = open(path)\n",
    "    word_dict = {}\n",
    "    for s_line in file.readlines():\n",
    "        spliter = re.compile(\"\\\\W+\")\n",
    "        words = spliter.split(s_line)\n",
    "        word_dict[words[0]] = int(words[1])\n",
    "    file.close()\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's implement a ham/spam email classifier. Please refer to the PDF file for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 words are:  ['nbsp', 'viagra', 'pills', 'cialis', 'voip', 'php', 'meds', 'computron', 'sex', 'ooking']\n"
     ]
    }
   ],
   "source": [
    "from likelihood import likelihood\n",
    "# TODO\n",
    "# Implement a ham/spam email classifier, and calculate the accuracy of your classifier\n",
    "\n",
    "# begin answer\n",
    "# a find the top 10 words\n",
    "p_cx_ham_train = np.zeros([1, N], np.float32)\n",
    "p_cx_spam_train = np.zeros([1, N], np.float32)\n",
    "\n",
    "p_cx_ham_train = (ham_train + 1) / (np.sum(ham_train + 1))\n",
    "p_cx_spam_train = (spam_train + 1) / (np.sum(spam_train + 1))\n",
    "\n",
    "ratio = []\n",
    "for i in range(0, N):\n",
    "    ratio.append(p_cx_spam_train[i] / p_cx_ham_train[i])\n",
    "    \n",
    "import heapq\n",
    "max_idx = heapq.nlargest(10, range(len(ratio)), ratio.__getitem__)\n",
    "max_idx = [i+1 for i in max_idx]\n",
    "word_dict = get_words_dict('all_word_map.txt')\n",
    "top_words_dict = {}\n",
    "for key, value in word_dict.items():\n",
    "    if value in max_idx:\n",
    "        top_words_dict[value] = key\n",
    "top_words_list = []\n",
    "for index in max_idx:\n",
    "    top_words_list.append(top_words_dict[index])\n",
    "print('top 10 words are: ', top_words_list)\n",
    "\n",
    "# end answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9857315598548972\n"
     ]
    }
   ],
   "source": [
    "# b the accuracy of  spam filter on the testing set\n",
    "# 计算垃圾邮件出现的概率\n",
    "import math\n",
    "from scipy.sparse import csr_matrix\n",
    "spam_positive_prob = num_spam_train / (num_spam_train + num_ham_train)\n",
    "\n",
    "ham_test = ham_test.toarray()\n",
    "spam_test = spam_test.toarray()\n",
    "\n",
    "correct_num = 0\n",
    "true_labels = []\n",
    "predict_labels = []\n",
    "for i in range(0, ham_test.shape[0]):\n",
    "    p_spam = np.sum(np.log(p_cx_spam_train) * ham_test[i]) + math.log(spam_positive_prob)\n",
    "    p_ham = np.sum(np.log(p_cx_ham_train) * ham_test[i]) + math.log(1 - spam_positive_prob)\n",
    "    if p_spam > p_ham:\n",
    "        predict_label = 1\n",
    "    else:\n",
    "        predict_label = 0\n",
    "    predict_labels.append(predict_label)\n",
    "    if predict_label == 0:\n",
    "        correct_num += 1        \n",
    "for i in range(0, spam_test.shape[0]):\n",
    "    p_spam = np.sum(np.log(p_cx_spam_train) * spam_test[i]) + math.log(spam_positive_prob)\n",
    "    p_ham = np.sum(np.log(p_cx_ham_train) * spam_test[i]) + math.log(1 - spam_positive_prob)\n",
    "    if p_spam > p_ham:\n",
    "        predict_label = 1\n",
    "    else:\n",
    "        predict_label = 0\n",
    "    predict_labels.append(predict_label)\n",
    "    if predict_label == 1:\n",
    "        correct_num += 1\n",
    "accuracy = (correct_num) / (len(spam_test) + len(ham_test))\n",
    "print('Accuracy: ', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp:  1093\n",
      "fp:  28\n",
      "Precision:  0.9750223015165032\n",
      "tp:  1093\n",
      "fn:  31\n",
      "Recall:  0.9724199288256228\n"
     ]
    }
   ],
   "source": [
    "true_labels.extend([0] * len(ham_test))\n",
    "true_labels.extend([1] * len(spam_test))\n",
    "true_labels = np.array(true_labels)\n",
    "predict_labels = np.array(predict_labels)\n",
    "\n",
    "tp = np.sum(np.logical_and(true_labels == 1, predict_labels == 1))\n",
    "fp = np.sum(np.logical_and(true_labels == 0, predict_labels == 1))\n",
    "print('tp: ', tp)\n",
    "print('fp: ', fp)\n",
    "precision = tp / (tp + fp)\n",
    "print('Precision: ', precision)\n",
    "\n",
    "tp = np.sum(np.logical_and(true_labels == 1, predict_labels == 1))\n",
    "fn = np.sum(np.logical_and(true_labels == 1, predict_labels == 0))\n",
    "print('tp: ', tp)\n",
    "print('fn: ', fn)\n",
    "recall = tp / (tp + fn)\n",
    "print('Recall: ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
